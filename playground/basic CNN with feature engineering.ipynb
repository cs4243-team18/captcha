{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5577091d-da5d-457a-b651-0d03d199522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Training Data: 100%|█████████████████████████████████████████████████████████| 7437/7437 [01:34<00:00, 78.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 9 features:\n",
      "  1. aspect_ratio\n",
      "  2. pixel_density\n",
      "  3. h_symmetry\n",
      "  4. v_symmetry\n",
      "  5. contour_count\n",
      "  6. h_proj_std\n",
      "  7. v_proj_std\n",
      "  8. com_x\n",
      "  9. com_y\n",
      "\n",
      "Building and training hybrid model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ feature_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ feature_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">196,736</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,608</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,644</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image_input (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m320\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling2d_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ feature_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │             \u001b[38;5;34m640\u001b[0m │ feature_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │         \u001b[38;5;34m196,736\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │           \u001b[38;5;34m2,080\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m20,608\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)                │           \u001b[38;5;34m4,644\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">280,452</span> (1.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m280,452\u001b[0m (1.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">280,452</span> (1.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m280,452\u001b[0m (1.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 36ms/step - accuracy: 0.1887 - loss: 2.9619 - val_accuracy: 0.6487 - val_loss: 1.1960\n",
      "Epoch 2/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.5723 - loss: 1.4611 - val_accuracy: 0.7196 - val_loss: 0.9072\n",
      "Epoch 3/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.6607 - loss: 1.1371 - val_accuracy: 0.7570 - val_loss: 0.7849\n",
      "Epoch 4/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step - accuracy: 0.7089 - loss: 0.9656 - val_accuracy: 0.7763 - val_loss: 0.7133\n",
      "Epoch 5/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.7366 - loss: 0.8765 - val_accuracy: 0.7955 - val_loss: 0.6563\n",
      "Epoch 6/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.7554 - loss: 0.8025 - val_accuracy: 0.8087 - val_loss: 0.6076\n",
      "Epoch 7/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.7671 - loss: 0.7591 - val_accuracy: 0.8164 - val_loss: 0.5877\n",
      "Epoch 8/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.7830 - loss: 0.7056 - val_accuracy: 0.8294 - val_loss: 0.5597\n",
      "Epoch 9/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.8002 - loss: 0.6529 - val_accuracy: 0.8345 - val_loss: 0.5485\n",
      "Epoch 10/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.7997 - loss: 0.6401 - val_accuracy: 0.8306 - val_loss: 0.5459\n",
      "Epoch 11/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.8134 - loss: 0.6042 - val_accuracy: 0.8390 - val_loss: 0.5199\n",
      "Epoch 12/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.8198 - loss: 0.5821 - val_accuracy: 0.8462 - val_loss: 0.5209\n",
      "Epoch 13/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.8268 - loss: 0.5648 - val_accuracy: 0.8459 - val_loss: 0.5039\n",
      "Epoch 14/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.8330 - loss: 0.5393 - val_accuracy: 0.8439 - val_loss: 0.5329\n",
      "Epoch 15/15\n",
      "\u001b[1m478/478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.8306 - loss: 0.5368 - val_accuracy: 0.8568 - val_loss: 0.5054\n",
      "\n",
      "Evaluating the model on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Data: 100%|█████████████████████████████████████████████████████████| 1894/1894 [19:17<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character-level Metrics:\n",
      "Accuracy: 0.7851 (8266/11340)\n",
      "Precision: 0.7926\n",
      "Recall: 0.7851\n",
      "F1 Score: 0.7860\n",
      "\n",
      "Captcha-level Metrics:\n",
      "Accuracy: 0.4324 (819/1894)\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Classification Report (Character-level):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       343\n",
      "           1       0.62      0.64      0.63       238\n",
      "           2       0.89      0.80      0.84       303\n",
      "           3       0.93      0.85      0.89       319\n",
      "           4       0.85      0.75      0.80       300\n",
      "           5       0.78      0.76      0.77       286\n",
      "           6       0.81      0.87      0.84       287\n",
      "           7       0.78      0.84      0.81       288\n",
      "           8       0.92      0.75      0.82       308\n",
      "           9       0.86      0.78      0.82       308\n",
      "           a       0.82      0.76      0.79       332\n",
      "           b       0.78      0.79      0.79       302\n",
      "           c       0.82      0.86      0.84       309\n",
      "           d       0.84      0.79      0.81       317\n",
      "           e       0.79      0.81      0.80       295\n",
      "           f       0.82      0.82      0.82       291\n",
      "           g       0.75      0.76      0.76       301\n",
      "           h       0.79      0.71      0.75       333\n",
      "           i       0.75      0.49      0.59       221\n",
      "           j       0.80      0.78      0.79       294\n",
      "           k       0.86      0.81      0.84       331\n",
      "           l       0.60      0.86      0.71       505\n",
      "           m       0.78      0.78      0.78       297\n",
      "           n       0.78      0.85      0.81       336\n",
      "           o       0.57      0.64      0.60       300\n",
      "           p       0.92      0.91      0.92       298\n",
      "           q       0.72      0.81      0.76       302\n",
      "           r       0.69      0.83      0.75       282\n",
      "           s       0.77      0.68      0.72       324\n",
      "           t       0.76      0.81      0.79       332\n",
      "           u       0.79      0.81      0.80       315\n",
      "           v       0.82      0.78      0.80       320\n",
      "           w       0.82      0.82      0.82       357\n",
      "           x       0.87      0.84      0.85       287\n",
      "           y       0.92      0.79      0.85       310\n",
      "           z       0.84      0.84      0.84       288\n",
      "\n",
      "    accuracy                           0.79     11159\n",
      "   macro avg       0.79      0.78      0.79     11159\n",
      "weighted avg       0.79      0.79      0.79     11159\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 526\u001b[0m\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  F1 Score:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptcha_f1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 526\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 490\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test data\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating the model on test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 490\u001b[0m metrics, results_df \u001b[38;5;241m=\u001b[39m evaluate_model(model, TEST_FOLDER_PATH, scaler)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# Show example predictions\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mShowing example predictions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 355\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_folder_path, scaler)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# plt.savefig('metrics_comparison.png')\u001b[39;00m\n\u001b[0;32m    344\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: char_accuracy,\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_precision\u001b[39m\u001b[38;5;124m'\u001b[39m: char_precision,\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_recall\u001b[39m\u001b[38;5;124m'\u001b[39m: char_recall,\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: char_f1,\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptcha_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: captcha_accuracy,\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptcha_precision\u001b[39m\u001b[38;5;124m'\u001b[39m: captcha_precision,\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptcha_recall\u001b[39m\u001b[38;5;124m'\u001b[39m: captcha_recall,\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaptcha_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: captcha_f1\n\u001b[1;32m--> 355\u001b[0m }, results_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from helper_functions.preprocessing import replace_black_with_median, remove_salt_and_pepper_noise\n",
    "from helper_functions.segmentation import segment_captcha_with_projection\n",
    "from helper_functions.extract_feature import extract_features\n",
    "\n",
    "# Tunable parameters\n",
    "TUNABLE_PARAMETERS = {\n",
    "    \"median_filter_kernel_size\": 7,\n",
    "    \"gaussian_blur_kernel_size\": (3, 3),\n",
    "    \"salt_pepper_kernel_size\": 1,\n",
    "    \"adaptive_threshold_block_size\": 31,\n",
    "    \"adaptive_threshold_C\": 2,\n",
    "    \"projection_threshold\": 0.1\n",
    "}\n",
    "\n",
    "# Paths\n",
    "TRAIN_FOLDER_PATH = \"../data/train/combine\"\n",
    "TEST_FOLDER_PATH = \"../data/test/combine\"\n",
    "MODEL_SAVE_PATH = \"captcha_model.h5\"\n",
    "CHAR_MODEL_SAVE_PATH = \"char_recognition_model.h5\"\n",
    "\n",
    "# CNN parameters\n",
    "IMG_HEIGHT = 40\n",
    "IMG_WIDTH = 30\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Possible characters in captchas (adjust if needed)\n",
    "CHARACTERS = string.ascii_lowercase + string.digits\n",
    "\n",
    "\n",
    "# Preprocess an image for segmentation\n",
    "def preprocess_image(image):\n",
    "    if len(image.shape) == 3:  # If color image\n",
    "        denoised = replace_black_with_median(image.copy(), TUNABLE_PARAMETERS['median_filter_kernel_size'])\n",
    "        gray = cv2.cvtColor(denoised, cv2.COLOR_BGR2GRAY)\n",
    "    else:  # If already grayscale\n",
    "        denoised = replace_black_with_median(image.copy(), TUNABLE_PARAMETERS['median_filter_kernel_size'])\n",
    "        gray = denoised\n",
    "    \n",
    "    denoised_after_noise_removal = remove_salt_and_pepper_noise(gray, TUNABLE_PARAMETERS['salt_pepper_kernel_size'])\n",
    "    blurred = cv2.GaussianBlur(denoised_after_noise_removal, TUNABLE_PARAMETERS['gaussian_blur_kernel_size'], 0)\n",
    "    \n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        blurred,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY_INV,\n",
    "        TUNABLE_PARAMETERS['adaptive_threshold_block_size'],\n",
    "        TUNABLE_PARAMETERS['adaptive_threshold_C']\n",
    "    )\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "\n",
    "# Prepare data for CNN training\n",
    "def prepare_training_data(folder_path):\n",
    "    \"\"\"\n",
    "    Denoise and tokenize captcha image files in a folder into individual characters using vertical projection.\n",
    "    Ignores image files where segmentation has failed (num_of_segmented_char =/= actual_num_of_char).\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing CAPTCHA images. Image files in the folder should be of format \"captchachars-0\" （e.g \"abc123-0\")\n",
    "\n",
    "    Returns:\n",
    "    tuple：\n",
    "    - X_img：Numpy array of Image of char with type numpy array (40 x 30)\n",
    "    - feature_list: Dictionary of features extracted from the corresponding image \n",
    "    - y: Numpy array of one-hot encoded label of the corresponding image\n",
    "    \"\"\"\n",
    "    all_images = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "    \n",
    "    X_img = []  # Images\n",
    "    X_features_list = []  # Engineered features\n",
    "    y = []  # Labels\n",
    "    \n",
    "    for filename in tqdm(all_images, desc=\"Preparing Training Data\"):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        filename_without_suffix = os.path.splitext(filename)[0]\n",
    "        correct_characters = filename_without_suffix.split('-')[0]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        thresh = preprocess_image(image)\n",
    "        \n",
    "        character_boundaries, _, _ = segment_captcha_with_projection(thresh, TUNABLE_PARAMETERS['projection_threshold'])\n",
    "        \n",
    "        # Skip if segmentation failed or number of segments doesn't match expected characters\n",
    "        if len(character_boundaries) != len(correct_characters):\n",
    "            continue\n",
    "        \n",
    "        for i, (start, end) in enumerate(character_boundaries):\n",
    "            char_image = thresh[:, start:end]\n",
    "            char_label = correct_characters[i]\n",
    "            \n",
    "            # Skip if character is not in our expected set\n",
    "            if char_label not in CHARACTERS:\n",
    "                continue\n",
    "            \n",
    "            # Extract features and resized image\n",
    "            features, char_image_resized = extract_features(char_image)\n",
    "            \n",
    "            # Add to dataset\n",
    "            X_img.append(char_image_resized)\n",
    "            X_features_list.append(features)\n",
    "            y.append(CHARACTERS.index(char_label))\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_img = np.array(X_img)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Convert features to DataFrame and then to numpy array\n",
    "    features_df = pd.DataFrame(X_features_list)\n",
    "    X_features = features_df.values\n",
    "    feature_names = list(features_df.columns)\n",
    "    \n",
    "    # Reshape for CNN input\n",
    "    X_img = X_img.reshape(X_img.shape[0], IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "    X_img = X_img / 255.0  # Normalize\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_one_hot = to_categorical(y, num_classes=len(CHARACTERS))\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_features_scaled = scaler.fit_transform(X_features)\n",
    "    \n",
    "    return X_img, X_features_scaled, y_one_hot, feature_names, scaler\n",
    "\n",
    "# Build hybrid model that combines CNN for images and features\n",
    "def build_hybrid_model(feature_count):\n",
    "    # Image input branch\n",
    "    img_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1), name='image_input')\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    img_features = layers.Dense(128, activation='relu')(x)\n",
    "    \n",
    "    # Engineered features input branch\n",
    "    feature_input = Input(shape=(feature_count,), name='feature_input')\n",
    "    feature_branch = layers.Dense(64, activation='relu')(feature_input)\n",
    "    feature_branch = layers.Dropout(0.3)(feature_branch)\n",
    "    feature_branch = layers.Dense(32, activation='relu')(feature_branch)\n",
    "    \n",
    "    # Combine both branches\n",
    "    combined = layers.concatenate([img_features, feature_branch])\n",
    "    combined = layers.Dense(128, activation='relu')(combined)\n",
    "    combined = layers.Dropout(0.5)(combined)\n",
    "    output = layers.Dense(len(CHARACTERS), activation='softmax')(combined)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[img_input, feature_input], outputs=output)\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluate the model on test data\n",
    "def evaluate_model(model, test_folder_path, scaler):\n",
    "    all_images = [f for f in os.listdir(test_folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "    \n",
    "    correct_chars = 0\n",
    "    total_chars = 0\n",
    "    correct_captchas = 0\n",
    "    total_captchas = 0\n",
    "    \n",
    "    all_true_chars = []\n",
    "    all_pred_chars = []\n",
    "    \n",
    "    # For captcha-level metrics\n",
    "    all_true_captchas = []\n",
    "    all_pred_captchas = []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for filename in tqdm(all_images, desc=\"Evaluating on Test Data\"):\n",
    "        image_path = os.path.join(test_folder_path, filename)\n",
    "        filename_without_suffix = os.path.splitext(filename)[0]\n",
    "        correct_characters = filename_without_suffix.split('-')[0]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        thresh = preprocess_image(image)\n",
    "        \n",
    "        character_boundaries, _, _ = segment_captcha_with_projection(thresh, TUNABLE_PARAMETERS['projection_threshold'])\n",
    "        \n",
    "        if len(character_boundaries) == 0:\n",
    "            total_captchas += 1\n",
    "            # Add to captcha-level metrics\n",
    "            all_true_captchas.append(correct_characters)\n",
    "            all_pred_captchas.append(\"\")  # Empty prediction\n",
    "            continue\n",
    "        \n",
    "        predicted_chars = []\n",
    "        \n",
    "        for i, (start, end) in enumerate(character_boundaries):\n",
    "            char_image = thresh[:, start:end]\n",
    "            \n",
    "            # Extract features and resized image\n",
    "            features, char_image_resized = extract_features(char_image)\n",
    "            \n",
    "            # Prepare image for prediction\n",
    "            char_image_resized = char_image_resized.reshape(1, IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "            char_image_resized = char_image_resized / 255.0\n",
    "            \n",
    "            # Prepare features for prediction\n",
    "            features_df = pd.DataFrame([features])\n",
    "            features_array = features_df.values\n",
    "            features_scaled = scaler.transform(features_array)\n",
    "            \n",
    "            # Predict using hybrid model\n",
    "            prediction = model.predict([char_image_resized, features_scaled], verbose=0)\n",
    "            predicted_idx = np.argmax(prediction)\n",
    "            predicted_char = CHARACTERS[predicted_idx]\n",
    "            \n",
    "            predicted_chars.append(predicted_char)\n",
    "            \n",
    "            # Add to character-level metrics\n",
    "            if i < len(correct_characters):\n",
    "                all_true_chars.append(correct_characters[i])\n",
    "                all_pred_chars.append(predicted_char)\n",
    "        \n",
    "        predicted_text = ''.join(predicted_chars)\n",
    "        \n",
    "        # Add to captcha-level metrics\n",
    "        all_true_captchas.append(correct_characters)\n",
    "        all_pred_captchas.append(predicted_text)\n",
    "        \n",
    "        # Count correct characters\n",
    "        if len(predicted_chars) == len(correct_characters):\n",
    "            for i in range(len(correct_characters)):\n",
    "                if predicted_chars[i] == correct_characters[i]:\n",
    "                    correct_chars += 1\n",
    "                total_chars += 1\n",
    "            \n",
    "            # Check if entire captcha is correct\n",
    "            if predicted_text == correct_characters:\n",
    "                correct_captchas += 1\n",
    "        else:\n",
    "            total_chars += len(correct_characters)\n",
    "        \n",
    "        total_captchas += 1\n",
    "        \n",
    "        # Store results for later analysis\n",
    "        results.append({\n",
    "            'filename': filename,\n",
    "            'true_text': correct_characters,\n",
    "            'predicted_text': predicted_text,\n",
    "            'correct': predicted_text == correct_characters,\n",
    "            'char_count': len(correct_characters),\n",
    "            'segments_found': len(character_boundaries)\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics for character-level evaluation\n",
    "    char_accuracy = accuracy_score(all_true_chars, all_pred_chars) if all_true_chars else 0\n",
    "    char_precision = precision_score(all_true_chars, all_pred_chars, average='weighted', zero_division=0)\n",
    "    char_recall = recall_score(all_true_chars, all_pred_chars, average='weighted', zero_division=0)\n",
    "    char_f1 = f1_score(all_true_chars, all_pred_chars, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate metrics for captcha-level evaluation\n",
    "    captcha_accuracy = sum([1 if true == pred else 0 for true, pred in zip(all_true_captchas, all_pred_captchas)]) / len(all_true_captchas)\n",
    "    \n",
    "    # For captcha-level precision, recall, and F1, we need to convert to binary classification\n",
    "    # (correct or incorrect captcha)\n",
    "    captcha_true_binary = [1 if true == pred else 0 for true, pred in zip(all_true_captchas, all_pred_captchas)]\n",
    "    captcha_pred_binary = [1 if true == pred else 0 for true, pred in zip(all_true_captchas, all_pred_captchas)]\n",
    "    \n",
    "    captcha_precision = precision_score(captcha_true_binary, captcha_pred_binary, zero_division=0)\n",
    "    captcha_recall = recall_score(captcha_true_binary, captcha_pred_binary, zero_division=0)\n",
    "    captcha_f1 = f1_score(captcha_true_binary, captcha_pred_binary, zero_division=0)\n",
    "    \n",
    "    # Print character-level metrics\n",
    "    print(\"\\nCharacter-level Metrics:\")\n",
    "    print(f\"Accuracy: {char_accuracy:.4f} ({correct_chars}/{total_chars})\")\n",
    "    print(f\"Precision: {char_precision:.4f}\")\n",
    "    print(f\"Recall: {char_recall:.4f}\")\n",
    "    print(f\"F1 Score: {char_f1:.4f}\")\n",
    "    \n",
    "    # Print captcha-level metrics\n",
    "    print(\"\\nCaptcha-level Metrics:\")\n",
    "    print(f\"Accuracy: {captcha_accuracy:.4f} ({correct_captchas}/{total_captchas})\")\n",
    "    print(f\"Precision: {captcha_precision:.4f}\")\n",
    "    print(f\"Recall: {captcha_recall:.4f}\")\n",
    "    print(f\"F1 Score: {captcha_f1:.4f}\")\n",
    "    \n",
    "    # Create confusion matrix for character recognition\n",
    "    char_labels = sorted(set(all_true_chars + all_pred_chars))\n",
    "    conf_matrix = confusion_matrix(all_true_chars, all_pred_chars, labels=char_labels)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=char_labels, yticklabels=char_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Character Recognition Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report (Character-level):\")\n",
    "    print(classification_report(all_true_chars, all_pred_chars))\n",
    "    \n",
    "    # Save results to CSV\n",
    "    # results_df = pd.DataFrame(results)\n",
    "    # results_df.to_csv('captcha_recognition_results.csv', index=False)\n",
    "    \n",
    "    # Create a metrics summary DataFrame\n",
    "    metrics_summary = pd.DataFrame({\n",
    "        'Level': ['Character', 'Captcha'],\n",
    "        'Accuracy': [char_accuracy, captcha_accuracy],\n",
    "        'Precision': [char_precision, captcha_precision],\n",
    "        'Recall': [char_recall, captcha_recall],\n",
    "        'F1_Score': [char_f1, captcha_f1]\n",
    "    })\n",
    "    \n",
    "    # Save metrics summary to CSV\n",
    "    # metrics_summary.to_csv('recognition_metrics_summary.csv', index=False)\n",
    "    \n",
    "    # Plot metrics comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "    char_metrics = [char_accuracy, char_precision, char_recall, char_f1]\n",
    "    captcha_metrics = [captcha_accuracy, captcha_precision, captcha_recall, captcha_f1]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, char_metrics, width, label='Character Level')\n",
    "    plt.bar(x + width/2, captcha_metrics, width, label='Captcha Level')\n",
    "    \n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Recognition Metrics Comparison')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('metrics_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'char_accuracy': char_accuracy,\n",
    "        'char_precision': char_precision,\n",
    "        'char_recall': char_recall,\n",
    "        'char_f1': char_f1,\n",
    "        'captcha_accuracy': captcha_accuracy,\n",
    "        'captcha_precision': captcha_precision,\n",
    "        'captcha_recall': captcha_recall,\n",
    "        'captcha_f1': captcha_f1\n",
    "    }, results_df\n",
    "\n",
    "# Display example predictions\n",
    "def show_example_predictions(model, test_folder_path, scaler, num_examples=5):\n",
    "    all_images = [f for f in os.listdir(test_folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "    selected_images = random.sample(all_images, min(num_examples, len(all_images)))\n",
    "    \n",
    "    for filename in selected_images:\n",
    "        image_path = os.path.join(test_folder_path, filename)\n",
    "        filename_without_suffix = os.path.splitext(filename)[0]\n",
    "        correct_characters = filename_without_suffix.split('-')[0]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        thresh = preprocess_image(image)\n",
    "        \n",
    "        character_boundaries, _, _ = segment_captcha_with_projection(thresh, TUNABLE_PARAMETERS['projection_threshold'])\n",
    "        \n",
    "        if len(character_boundaries) == 0:\n",
    "            print(f\"No characters detected in {filename}.\")\n",
    "            continue\n",
    "        \n",
    "        predicted_chars = []\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Original: {correct_characters}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(thresh, cmap='gray')\n",
    "        plt.title(\"Preprocessed\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(12, 3))\n",
    "        for i, (start, end) in enumerate(character_boundaries):\n",
    "            char_image = thresh[:, start:end]\n",
    "            \n",
    "            # Extract features and resized image\n",
    "            features, char_image_resized = extract_features(char_image)\n",
    "            \n",
    "            # Prepare image for prediction\n",
    "            char_image_input = char_image_resized.reshape(1, IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "            char_image_input = char_image_input / 255.0\n",
    "            \n",
    "            # Prepare features for prediction\n",
    "            features_df = pd.DataFrame([features])\n",
    "            features_array = features_df.values\n",
    "            features_scaled = scaler.transform(features_array)\n",
    "            \n",
    "            # Predict using hybrid model\n",
    "            prediction = model.predict([char_image_input, features_scaled], verbose=0)\n",
    "            predicted_idx = np.argmax(prediction)\n",
    "            predicted_char = CHARACTERS[predicted_idx]\n",
    "            confidence = prediction[0][predicted_idx]\n",
    "            \n",
    "            predicted_chars.append(predicted_char)\n",
    "            \n",
    "            # Display segment and prediction\n",
    "            plt.subplot(1, len(character_boundaries), i+1)\n",
    "            plt.imshow(char_image, cmap='gray')\n",
    "            \n",
    "            true_char = correct_characters[i] if i < len(correct_characters) else \"?\"\n",
    "            title = f\"True: {true_char}\\nPred: {predicted_char}\\nConf: {confidence:.2f}\"\n",
    "            plt.title(title)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Image: {filename}\")\n",
    "        print(f\"True Text: {correct_characters}\")\n",
    "        print(f\"Predicted: {''.join(predicted_chars)}\")\n",
    "        print(f\"Correct: {'Yes' if ''.join(predicted_chars) == correct_characters else 'No'}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Prepare training data\n",
    "    print(\"Preparing training data...\")\n",
    "    X_img, X_features, y, feature_names, scaler = prepare_training_data(TRAIN_FOLDER_PATH)\n",
    "    \n",
    "    # Print feature information\n",
    "    print(f\"\\nExtracted {len(feature_names)} features:\")\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        print(f\"  {i+1}. {feature_name}\")\n",
    "    \n",
    "    # Build and train hybrid model\n",
    "    print(\"\\nBuilding and training hybrid model...\")\n",
    "    model = build_hybrid_model(X_features.shape[1])\n",
    "    \n",
    "    # Display model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_img, X_features], y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    # model.save(CHAR_MODEL_SAVE_PATH)\n",
    "    # print(f\"Model saved to {CHAR_MODEL_SAVE_PATH}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate the model on test data\n",
    "    print(\"\\nEvaluating the model on test data...\")\n",
    "    metrics, results_df = evaluate_model(model, TEST_FOLDER_PATH, scaler)\n",
    "    \n",
    "    # Show example predictions\n",
    "    print(\"\\nShowing example predictions...\")\n",
    "    show_example_predictions(model, TEST_FOLDER_PATH, scaler, num_examples=5)\n",
    "    \n",
    "    # Analyze segmentation performance\n",
    "    segmentation_success_rate = (results_df['segments_found'] == results_df['char_count']).mean()\n",
    "    print(f\"\\nSegmentation Success Rate: {segmentation_success_rate:.4f}\")\n",
    "    \n",
    "    # Analyze most common errors\n",
    "    incorrect_predictions = results_df[results_df['correct'] == False]\n",
    "    if len(incorrect_predictions) > 0:\n",
    "        print(\"\\nMost common error cases:\")\n",
    "        for _, row in incorrect_predictions.head(5).iterrows():\n",
    "            print(f\"  Filename: {row['filename']}\")\n",
    "            print(f\"  True: {row['true_text']}\")\n",
    "            print(f\"  Predicted: {row['predicted_text']}\")\n",
    "            print(f\"  Segments found: {row['segments_found']}\")\n",
    "            print(\"  \" + \"-\" * 30)\n",
    "    \n",
    "    # Print overall performance summary\n",
    "    print(\"\\nOverall Performance Summary:\")\n",
    "    print(\"Character-level metrics:\")\n",
    "    print(f\"  Accuracy:  {metrics['char_accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['char_precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['char_recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {metrics['char_f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nCaptcha-level metrics:\")\n",
    "    print(f\"  Accuracy:  {metrics['captcha_accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['captcha_precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['captcha_recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {metrics['captcha_f1']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44922a9-495d-418d-b1e6-0cc8873f8a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
